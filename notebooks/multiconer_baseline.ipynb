{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiconer_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIySQWQw031pD0DAYwLXv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuiyanmobasshir94/MultiCoNER/blob/main/notebooks/multiconer_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash \n",
        "mkdir model utils"
      ],
      "metadata": {
        "id": "1WFonIQ8tFjI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BJCL5qasuE9",
        "outputId": "e550790e-770d-4d8a-9e8d-da1431b42fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%file model/__init__.py\n",
        "\n",
        "## empty file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file model/ner_model.py\n",
        "\n",
        "from typing import List, Any\n",
        "\n",
        "import pytorch_lightning.core.lightning as pl\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from allennlp.modules import ConditionalRandomField\n",
        "from allennlp.modules.conditional_random_field import allowed_transitions\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup, AutoModel\n",
        "\n",
        "from log import logger\n",
        "from utils.metric import SpanF1\n",
        "from utils.reader_utils import extract_spans, get_tags\n",
        "\n",
        "\n",
        "class NERBaseAnnotator(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 train_data=None,\n",
        "                 dev_data=None,\n",
        "                 lr=1e-5,\n",
        "                 dropout_rate=0.1,\n",
        "                 batch_size=16,\n",
        "                 tag_to_id=None,\n",
        "                 stage='fit',\n",
        "                 pad_token_id=1,\n",
        "                 encoder_model='xlm-roberta-large',\n",
        "                 num_gpus=1):\n",
        "        super(NERBaseAnnotator, self).__init__()\n",
        "\n",
        "        self.train_data = train_data\n",
        "        self.dev_data = dev_data\n",
        "\n",
        "        self.id_to_tag = {v: k for k, v in tag_to_id.items()}\n",
        "        self.tag_to_id = tag_to_id\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.stage = stage\n",
        "        self.num_gpus = num_gpus\n",
        "        self.target_size = len(self.id_to_tag)\n",
        "\n",
        "        # set the default baseline model here\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        self.encoder_model = encoder_model\n",
        "        self.encoder = AutoModel.from_pretrained(encoder_model, return_dict=True)\n",
        "\n",
        "        self.feedforward = nn.Linear(in_features=self.encoder.config.hidden_size, out_features=self.target_size)\n",
        "        self.crf_layer = ConditionalRandomField(num_tags=self.target_size, constraints=allowed_transitions(constraint_type=\"BIO\", labels=self.id_to_tag))\n",
        "\n",
        "        self.lr = lr\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.span_f1 = SpanF1()\n",
        "        self.setup_model(self.stage)\n",
        "        self.save_hyperparameters('pad_token_id', 'encoder_model')\n",
        "\n",
        "    def setup_model(self, stage_name):\n",
        "        if stage_name == 'fit' and self.train_data is not None:\n",
        "            # Calculate total steps\n",
        "            train_batches = len(self.train_data) // (self.batch_size * self.num_gpus)\n",
        "            self.total_steps = 50 * train_batches\n",
        "\n",
        "            self.warmup_steps = int(self.total_steps * 0.01)\n",
        "\n",
        "    def collate_batch(self, batch):\n",
        "        batch_ = list(zip(*batch))\n",
        "        tokens, masks, gold_spans, tags = batch_[0], batch_[1], batch_[2], batch_[3]\n",
        "\n",
        "        max_len = max([len(token) for token in tokens])\n",
        "        token_tensor = torch.empty(size=(len(tokens), max_len), dtype=torch.long).fill_(self.pad_token_id)\n",
        "        tag_tensor = torch.empty(size=(len(tokens), max_len), dtype=torch.long).fill_(self.tag_to_id['O'])\n",
        "        mask_tensor = torch.zeros(size=(len(tokens), max_len), dtype=torch.bool)\n",
        "\n",
        "        for i in range(len(tokens)):\n",
        "            tokens_ = tokens[i]\n",
        "            seq_len = len(tokens_)\n",
        "\n",
        "            token_tensor[i, :seq_len] = tokens_\n",
        "            tag_tensor[i, :seq_len] = tags[i]\n",
        "            mask_tensor[i, :seq_len] = masks[i]\n",
        "\n",
        "        return token_tensor, tag_tensor, mask_tensor, gold_spans\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=0.01)\n",
        "        if self.stage == 'fit':\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.total_steps)\n",
        "            scheduler = {\n",
        "                'scheduler': scheduler,\n",
        "                'interval': 'step',\n",
        "                'frequency': 1\n",
        "            }\n",
        "            return [optimizer], [scheduler]\n",
        "        return [optimizer]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        loader = DataLoader(self.train_data, batch_size=self.batch_size, collate_fn=self.collate_batch, num_workers=10)\n",
        "        return loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        if self.dev_data is None:\n",
        "            return None\n",
        "        loader = DataLoader(self.dev_data, batch_size=self.batch_size, collate_fn=self.collate_batch, num_workers=10)\n",
        "        return loader\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        pred_results = self.span_f1.get_metric()\n",
        "        avg_loss = np.mean([preds['loss'].item() for preds in outputs])\n",
        "        self.log_metrics(pred_results, loss=avg_loss, on_step=False, on_epoch=True)\n",
        "\n",
        "        out = {\"test_loss\": avg_loss, \"results\": pred_results}\n",
        "        return out\n",
        "\n",
        "    def training_epoch_end(self, outputs: List[Any]) -> None:\n",
        "        pred_results = self.span_f1.get_metric(True)\n",
        "        avg_loss = np.mean([preds['loss'].item() for preds in outputs])\n",
        "        self.log_metrics(pred_results, loss=avg_loss, suffix='', on_step=False, on_epoch=True)\n",
        "\n",
        "    def validation_epoch_end(self, outputs: List[Any]) -> None:\n",
        "        pred_results = self.span_f1.get_metric(True)\n",
        "        avg_loss = np.mean([preds['loss'].item() for preds in outputs])\n",
        "        self.log_metrics(pred_results, loss=avg_loss, suffix='val_', on_step=False, on_epoch=True)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        output = self.perform_forward_step(batch)\n",
        "        self.log_metrics(output['results'], loss=output['loss'], suffix='val_', on_step=True, on_epoch=False)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        output = self.perform_forward_step(batch)\n",
        "        self.log_metrics(output['results'], loss=output['loss'], suffix='', on_step=True, on_epoch=False)\n",
        "        return output\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        output = self.perform_forward_step(batch, mode=self.stage)\n",
        "        self.log_metrics(output['results'], loss=output['loss'], suffix='_t', on_step=True, on_epoch=False)\n",
        "        return output\n",
        "\n",
        "    def log_metrics(self, pred_results, loss=0.0, suffix='', on_step=False, on_epoch=True):\n",
        "        for key in pred_results:\n",
        "            self.log(suffix + key, pred_results[key], on_step=on_step, on_epoch=on_epoch, prog_bar=True, logger=True)\n",
        "\n",
        "        self.log(suffix + 'loss', loss, on_step=on_step, on_epoch=on_epoch, prog_bar=True, logger=True)\n",
        "\n",
        "    def perform_forward_step(self, batch, mode=''):\n",
        "        tokens, tags, token_mask, metadata = batch\n",
        "        batch_size = tokens.size(0)\n",
        "\n",
        "        embedded_text_input = self.encoder(input_ids=tokens, attention_mask=token_mask)\n",
        "        embedded_text_input = embedded_text_input.last_hidden_state\n",
        "        embedded_text_input = self.dropout(F.leaky_relu(embedded_text_input))\n",
        "\n",
        "        # project the token representation for classification\n",
        "        token_scores = self.feedforward(embedded_text_input)\n",
        "\n",
        "        # compute the log-likelihood loss and compute the best NER annotation sequence\n",
        "        output = self._compute_token_tags(token_scores=token_scores, tags=tags, token_mask=token_mask, metadata=metadata, batch_size=batch_size, mode=mode)\n",
        "        return output\n",
        "\n",
        "    def _compute_token_tags(self, token_scores, tags, token_mask, metadata, batch_size, mode=''):\n",
        "        # compute the log-likelihood loss and compute the best NER annotation sequence\n",
        "        loss = -self.crf_layer(token_scores, tags, token_mask) / float(batch_size)\n",
        "        best_path = self.crf_layer.viterbi_tags(token_scores, token_mask)\n",
        "\n",
        "        pred_results, pred_tags = [], []\n",
        "        for i in range(batch_size):\n",
        "            tag_seq, _ = best_path[i]\n",
        "            pred_tags.append([self.id_to_tag[x] for x in tag_seq])\n",
        "            pred_results.append(extract_spans([self.id_to_tag[x] for x in tag_seq if x in self.id_to_tag]))\n",
        "\n",
        "        self.span_f1(pred_results, metadata)\n",
        "        output = {\"loss\": loss, \"results\": self.span_f1.get_metric()}\n",
        "\n",
        "        if mode == 'predict':\n",
        "            output['token_tags'] = pred_tags\n",
        "        return output\n",
        "\n",
        "    def predict_tags(self, batch, tokenizer=None):\n",
        "        tokens, tags, token_mask, metadata = batch\n",
        "        pred_tags = self.perform_forward_step(batch, mode='predict')['token_tags']\n",
        "        token_results, tag_results = [], []\n",
        "        for i in range(tokens.size(0)):\n",
        "            instance_token_results, instance_tag_results = get_tags(tokens[i], pred_tags[i], tokenizer=tokenizer)\n",
        "            token_results.append(instance_token_results)\n",
        "            tag_results.append(instance_tag_results)\n",
        "        return token_results, tag_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_mCx9AYs7QB",
        "outputId": "f988c04e-372d-4e74-ac31-b56e5c2b4d45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model/ner_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file utils/__init__.py\n",
        "\n",
        "## empty file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhVSmnc_tjWa",
        "outputId": "727ebdc9-d85d-413e-c852-64bba70bbf63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file utils/metric.py \n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import Set\n",
        "from overrides import overrides\n",
        "\n",
        "from allennlp.training.metrics.metric import Metric\n",
        "\n",
        "\n",
        "class SpanF1(Metric):\n",
        "    def __init__(self, non_entity_labels=['O']) -> None:\n",
        "        self._num_gold_mentions = 0\n",
        "        self._num_recalled_mentions = 0\n",
        "        self._num_predicted_mentions = 0\n",
        "        self._TP, self._FP, self._GT = defaultdict(int), defaultdict(int), defaultdict(int)\n",
        "        self.non_entity_labels = set(non_entity_labels)\n",
        "\n",
        "    @overrides\n",
        "    def __call__(self, batched_predicted_spans, batched_gold_spans, sentences=None):\n",
        "        non_entity_labels = self.non_entity_labels\n",
        "\n",
        "        for predicted_spans, gold_spans in zip(batched_predicted_spans, batched_gold_spans):\n",
        "            gold_spans_set = set([x for x, y in gold_spans.items() if y not in non_entity_labels])\n",
        "            pred_spans_set = set([x for x, y in predicted_spans.items() if y not in non_entity_labels])\n",
        "\n",
        "            self._num_gold_mentions += len(gold_spans_set)\n",
        "            self._num_recalled_mentions += len(gold_spans_set & pred_spans_set)\n",
        "            self._num_predicted_mentions += len(pred_spans_set)\n",
        "\n",
        "            for ky, val in gold_spans.items():\n",
        "                if val not in non_entity_labels:\n",
        "                    self._GT[val] += 1\n",
        "\n",
        "            for ky, val in predicted_spans.items():\n",
        "                if val in non_entity_labels:\n",
        "                    continue\n",
        "                if ky in gold_spans and val == gold_spans[ky]:\n",
        "                    self._TP[val] += 1\n",
        "                else:\n",
        "                    self._FP[val] += 1\n",
        "\n",
        "    @overrides\n",
        "    def get_metric(self, reset: bool = False) -> float:\n",
        "        all_tags: Set[str] = set()\n",
        "        all_tags.update(self._TP.keys())\n",
        "        all_tags.update(self._FP.keys())\n",
        "        all_tags.update(self._GT.keys())\n",
        "        all_metrics = {}\n",
        "\n",
        "        for tag in all_tags:\n",
        "            precision, recall, f1_measure = self.compute_prf_metrics(true_positives=self._TP[tag],\n",
        "                                                                     false_negatives=self._GT[tag] - self._TP[tag],\n",
        "                                                                     false_positives=self._FP[tag])\n",
        "            all_metrics['P@{}'.format(tag)] = precision\n",
        "            all_metrics['R@{}'.format(tag)] = recall\n",
        "            all_metrics['F1@{}'.format(tag)] = f1_measure\n",
        "\n",
        "        # Compute the precision, recall and f1 for all spans jointly.\n",
        "        precision, recall, f1_measure = self.compute_prf_metrics(true_positives=sum(self._TP.values()),\n",
        "                                                                 false_positives=sum(self._FP.values()),\n",
        "                                                                 false_negatives=sum(self._GT.values())-sum(self._TP.values()))\n",
        "        all_metrics[\"micro@P\"] = precision\n",
        "        all_metrics[\"micro@R\"] = recall\n",
        "        all_metrics[\"micro@F1\"] = f1_measure\n",
        "\n",
        "        if self._num_gold_mentions == 0:\n",
        "            entity_recall = 0.0\n",
        "        else:\n",
        "            entity_recall = self._num_recalled_mentions / float(self._num_gold_mentions)\n",
        "\n",
        "        if self._num_predicted_mentions == 0:\n",
        "            entity_precision = 0.0\n",
        "        else:\n",
        "            entity_precision = self._num_recalled_mentions / float(self._num_predicted_mentions)\n",
        "\n",
        "        all_metrics['MD@R'] = entity_recall\n",
        "        all_metrics['MD@P'] = entity_precision\n",
        "        all_metrics['MD@F1'] = 2. * ((entity_precision * entity_recall) / (entity_precision + entity_recall + 1e-13))\n",
        "        all_metrics['ALLTRUE'] = self._num_gold_mentions\n",
        "        all_metrics['ALLRECALLED'] = self._num_recalled_mentions\n",
        "        all_metrics['ALLPRED'] = self._num_predicted_mentions\n",
        "        if reset:\n",
        "            self.reset()\n",
        "        return all_metrics\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_prf_metrics(true_positives: int, false_positives: int, false_negatives: int):\n",
        "        precision = float(true_positives) / float(true_positives + false_positives + 1e-13)\n",
        "        recall = float(true_positives) / float(true_positives + false_negatives + 1e-13)\n",
        "        f1_measure = 2. * ((precision * recall) / (precision + recall + 1e-13))\n",
        "        return precision, recall, f1_measure\n",
        "\n",
        "    @overrides\n",
        "    def reset(self):\n",
        "        self._num_gold_mentions = 0\n",
        "        self._num_recalled_mentions = 0\n",
        "        self._num_predicted_mentions = 0\n",
        "        self._TP.clear()\n",
        "        self._FP.clear()\n",
        "        self._GT.clear()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8yqJ9rDts_P",
        "outputId": "1b0b7afc-576e-459e-c1d2-9467649f5786"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils/metric.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file utils/reader.py \n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from log import logger\n",
        "from utils.reader_utils import get_ner_reader, extract_spans, _assign_ner_tags\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "\n",
        "class CoNLLReader(Dataset):\n",
        "    def __init__(self, max_instances=-1, max_length=50, target_vocab=None, pretrained_dir='', encoder_model='xlm-roberta-large'):\n",
        "        self._max_instances = max_instances\n",
        "        self._max_length = max_length\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_dir + encoder_model)\n",
        "\n",
        "        self.pad_token = self.tokenizer.special_tokens_map['pad_token']\n",
        "        self.pad_token_id = self.tokenizer.get_vocab()[self.pad_token]\n",
        "        self.sep_token = self.tokenizer.special_tokens_map['sep_token']\n",
        "\n",
        "        self.label_to_id = {} if target_vocab is None else target_vocab\n",
        "        self.instances = []\n",
        "\n",
        "    def get_target_size(self):\n",
        "        return len(set(self.label_to_id.values()))\n",
        "\n",
        "    def get_target_vocab(self):\n",
        "        return self.label_to_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.instances[item]\n",
        "\n",
        "    def read_data(self, data):\n",
        "        dataset_name = data if isinstance(data, str) else 'dataframe'\n",
        "        logger.info('Reading file {}'.format(dataset_name))\n",
        "        instance_idx = 0\n",
        "\n",
        "        for fields, metadata in get_ner_reader(data=data):\n",
        "            if self._max_instances != -1 and instance_idx > self._max_instances:\n",
        "                break\n",
        "            sentence_str, tokens_sub_rep, token_masks_rep, coded_ner_, gold_spans_ = self.parse_line_for_ner(fields=fields)\n",
        "\n",
        "            tokens_tensor = torch.tensor(tokens_sub_rep, dtype=torch.long)\n",
        "            tag_tensor = torch.tensor(coded_ner_, dtype=torch.long).unsqueeze(0)\n",
        "            token_masks_rep = torch.tensor(token_masks_rep)\n",
        "\n",
        "            self.instances.append((tokens_tensor, token_masks_rep, gold_spans_, tag_tensor))\n",
        "            instance_idx += 1\n",
        "        logger.info('Finished reading {:d} instances from file {}'.format(len(self.instances), dataset_name))\n",
        "\n",
        "    def parse_line_for_ner(self, fields):\n",
        "        tokens_, ner_tags = fields[0], fields[-1]\n",
        "        sentence_str, tokens_sub_rep, ner_tags_rep, token_masks_rep = self.parse_tokens_for_ner(tokens_, ner_tags)\n",
        "        gold_spans_ = extract_spans(ner_tags_rep)\n",
        "        coded_ner_ = [self.label_to_id[tag] for tag in ner_tags_rep]\n",
        "\n",
        "        return sentence_str, tokens_sub_rep, token_masks_rep, coded_ner_, gold_spans_\n",
        "\n",
        "    def parse_tokens_for_ner(self, tokens_, ner_tags):\n",
        "        sentence_str = ''\n",
        "        tokens_sub_rep, ner_tags_rep = [self.pad_token_id], ['O']\n",
        "\n",
        "        for idx, token in enumerate(tokens_):\n",
        "            if self._max_length != -1 and len(tokens_sub_rep) > self._max_length:\n",
        "                break\n",
        "            sentence_str += ' ' + ' '.join(self.tokenizer.tokenize(token.lower()))\n",
        "            rep_ = self.tokenizer(token.lower())['input_ids']\n",
        "            rep_ = rep_[1:-1]\n",
        "            tokens_sub_rep.extend(rep_)\n",
        "\n",
        "            # if we have a NER here, in the case of B, the first NER tag is the B tag, the rest are I tags.\n",
        "            ner_tag = ner_tags[idx]\n",
        "            tags, masks = _assign_ner_tags(ner_tag, rep_)\n",
        "            ner_tags_rep.extend(tags)\n",
        "\n",
        "        tokens_sub_rep.append(self.pad_token_id)\n",
        "        ner_tags_rep.append('O')\n",
        "        token_masks_rep = [True] * len(tokens_sub_rep)\n",
        "        return sentence_str, tokens_sub_rep, ner_tags_rep, "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SiznCekt6Sy",
        "outputId": "4eb9e911-a335-4d65-ae5e-2c369cd2410b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils/reader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file utils/reader_utils.py\n",
        "\n",
        "import gzip\n",
        "import itertools\n",
        "\n",
        "\n",
        "def get_ner_reader(data):\n",
        "    fin = gzip.open(data, 'rt') if data.endswith('.gz') else open(data, 'rt')\n",
        "    for is_divider, lines in itertools.groupby(fin, _is_divider):\n",
        "        if is_divider:\n",
        "            continue\n",
        "        lines = [line.strip().replace('\\u200d', '').replace('\\u200c', '') for line in lines]\n",
        "\n",
        "        metadata = lines[0].strip() if lines[0].strip().startswith('# id') else None\n",
        "        fields = [line.split() for line in lines if not line.startswith('# id')]\n",
        "        fields = [list(field) for field in zip(*fields)]\n",
        "\n",
        "\n",
        "        yield fields, metadata\n",
        "\n",
        "\n",
        "def _assign_ner_tags(ner_tag, rep_):\n",
        "    ner_tags_rep = []\n",
        "    token_masks = []\n",
        "\n",
        "    sub_token_len = len(rep_)\n",
        "    token_masks.extend([True] * sub_token_len)\n",
        "    if ner_tag[0] == 'B':\n",
        "        in_tag = 'I' + ner_tag[1:]\n",
        "\n",
        "        ner_tags_rep.append(ner_tag)\n",
        "        ner_tags_rep.extend([in_tag] * (sub_token_len - 1))\n",
        "    else:\n",
        "        ner_tags_rep.extend([ner_tag] * sub_token_len)\n",
        "    return ner_tags_rep, token_masks\n",
        "\n",
        "\n",
        "def extract_spans(tags):\n",
        "    cur_tag = None\n",
        "    cur_start = None\n",
        "    gold_spans = {}\n",
        "\n",
        "    def _save_span(_cur_tag, _cur_start, _cur_id, _gold_spans):\n",
        "        if _cur_start is None:\n",
        "            return _gold_spans\n",
        "        _gold_spans[(_cur_start, _cur_id - 1)] = _cur_tag  # inclusive start & end, accord with conll-coref settings\n",
        "        return _gold_spans\n",
        "\n",
        "    # iterate over the tags\n",
        "    for _id, nt in enumerate(tags):\n",
        "        indicator = nt[0]\n",
        "        if indicator == 'B':\n",
        "            gold_spans = _save_span(cur_tag, cur_start, _id, gold_spans)\n",
        "            cur_start = _id\n",
        "            cur_tag = nt[2:]\n",
        "            pass\n",
        "        elif indicator == 'I':\n",
        "            # do nothing\n",
        "            pass\n",
        "        elif indicator == 'O':\n",
        "            gold_spans = _save_span(cur_tag, cur_start, _id, gold_spans)\n",
        "            cur_tag = 'O'\n",
        "            cur_start = _id\n",
        "            pass\n",
        "    _save_span(cur_tag, cur_start, _id + 1, gold_spans)\n",
        "    return gold_spans\n",
        "\n",
        "\n",
        "def _is_divider(line: str) -> bool:\n",
        "    empty_line = line.strip() == ''\n",
        "    if empty_line:\n",
        "        return True\n",
        "\n",
        "    first_token = line.split()[0]\n",
        "    if first_token == \"-DOCSTART-\":# or line.startswith('# id'):  # pylint: disable=simplifiable-if-statement\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_tags(tokens, tags, tokenizer=None, start_token_pattern='▁'):\n",
        "    token_results, tag_results = [], []\n",
        "    index = 0\n",
        "    token_word = []\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokens)\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if token == tokenizer.pad_token:\n",
        "            # index += 1\n",
        "            continue\n",
        "\n",
        "        if index == 0:\n",
        "            tag_results.append(tag)\n",
        "\n",
        "        elif token.startswith(start_token_pattern) and token != '▁́':\n",
        "            tag_results.append(tag)\n",
        "\n",
        "            if tokenizer is not None:\n",
        "                token_results.append(''.join(token_word).replace(start_token_pattern, ''))\n",
        "            token_word.clear()\n",
        "\n",
        "        token_word.append(token)\n",
        "\n",
        "        index += 1\n",
        "    token_results.append(''.join(token_word).replace(start_token_pattern, ''))\n",
        "\n",
        "    return token_results, tag_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYNAe8AiuMZh",
        "outputId": "1e188521-f03b-4e1b-e5d4-1962d9e70527"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils/reader_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file utils/utils.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, EarlyStopping\n",
        "\n",
        "from log import logger\n",
        "from model.ner_model import NERBaseAnnotator\n",
        "from utils.reader import CoNLLReader\n",
        "\n",
        "conll_iob = {'B-ORG': 0, 'I-ORG': 1, 'B-MISC': 2, 'I-MISC': 3, 'B-LOC': 4, 'I-LOC': 5, 'B-PER': 6, 'I-PER': 7, 'O': 8}\n",
        "wnut_iob = {'B-CORP': 0, 'I-CORP': 1, 'B-CW': 2, 'I-CW': 3, 'B-GRP': 4, 'I-GRP': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-PER': 8, 'I-PER': 9, 'B-PROD': 10, 'I-PROD': 11, 'O': 12}\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description='Model configuration.', add_help=False)\n",
        "    p.add_argument('--train', type=str, help='Path to the train data.', default=None)\n",
        "    p.add_argument('--test', type=str, help='Path to the test data.', default=None)\n",
        "    p.add_argument('--dev', type=str, help='Path to the dev data.', default=None)\n",
        "\n",
        "    p.add_argument('--out_dir', type=str, help='Output directory.', default='.')\n",
        "    p.add_argument('--iob_tagging', type=str, help='IOB tagging scheme', default='wnut')\n",
        "\n",
        "    p.add_argument('--max_instances', type=int, help='Maximum number of instances', default=-1)\n",
        "    p.add_argument('--max_length', type=int, help='Maximum number of tokens per instance.', default=50)\n",
        "\n",
        "    p.add_argument('--encoder_model', type=str, help='Pretrained encoder model to use', default='xlm-roberta-large')\n",
        "    p.add_argument('--model', type=str, help='Model path.', default=None)\n",
        "    p.add_argument('--model_name', type=str, help='Model name.', default=None)\n",
        "    p.add_argument('--stage', type=str, help='Training stage', default='fit')\n",
        "    p.add_argument('--prefix', type=str, help='Prefix for storing evaluation files.', default='test')\n",
        "\n",
        "    p.add_argument('--batch_size', type=int, help='Batch size.', default=128)\n",
        "    p.add_argument('--gpus', type=int, help='Number of GPUs.', default=1)\n",
        "    p.add_argument('--epochs', type=int, help='Number of epochs for training.', default=5)\n",
        "    p.add_argument('--lr', type=float, help='Learning rate', default=1e-5)\n",
        "    p.add_argument('--dropout', type=float, help='Dropout rate', default=0.1)\n",
        "\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def get_tagset(tagging_scheme):\n",
        "    if 'conll' in tagging_scheme:\n",
        "        return conll_iob\n",
        "    return wnut_iob\n",
        "\n",
        "\n",
        "def get_out_filename(out_dir, model, prefix):\n",
        "    model_name = os.path.basename(model)\n",
        "    model_name = model_name[:model_name.rfind('.')]\n",
        "    return '{}/{}_base_{}.tsv'.format(out_dir, prefix, model_name)\n",
        "\n",
        "\n",
        "def write_eval_performance(eval_performance, out_file):\n",
        "    outstr = ''\n",
        "    added_keys = set()\n",
        "    for out_ in eval_performance:\n",
        "        for k in out_:\n",
        "            if k in added_keys or k in ['results', 'predictions']:\n",
        "                continue\n",
        "            outstr = outstr + '{}\\t{}\\n'.format(k, out_[k])\n",
        "            added_keys.add(k)\n",
        "\n",
        "    open(out_file, 'wt').write(outstr)\n",
        "    logger.info('Finished writing evaluation performance for {}'.format(out_file))\n",
        "\n",
        "\n",
        "def get_reader(file_path, max_instances=-1, max_length=50, target_vocab=None, encoder_model='xlm-roberta-large'):\n",
        "    if file_path is None:\n",
        "        return None\n",
        "    reader = CoNLLReader(max_instances=max_instances, max_length=max_length, target_vocab=target_vocab, encoder_model=encoder_model)\n",
        "    reader.read_data(file_path)\n",
        "\n",
        "    return reader\n",
        "\n",
        "\n",
        "def create_model(train_data, dev_data, tag_to_id, batch_size=64, dropout_rate=0.1, stage='fit', lr=1e-5, encoder_model='xlm-roberta-large', num_gpus=1):\n",
        "    return NERBaseAnnotator(train_data=train_data, dev_data=dev_data, tag_to_id=tag_to_id, batch_size=batch_size, stage=stage, encoder_model=encoder_model,\n",
        "                            dropout_rate=dropout_rate, lr=lr, pad_token_id=train_data.pad_token_id, num_gpus=num_gpus)\n",
        "\n",
        "\n",
        "def load_model(model_file, tag_to_id=None, stage='test'):\n",
        "    if ~os.path.isfile(model_file):\n",
        "        model_file = get_models_for_evaluation(model_file)\n",
        "\n",
        "    hparams_file = model_file[:model_file.rindex('checkpoints/')] + '/hparams.yaml'\n",
        "    model = NERBaseAnnotator.load_from_checkpoint(model_file, hparams_file=hparams_file, stage=stage, tag_to_id=tag_to_id)\n",
        "    model.stage = stage\n",
        "    return model, model_file\n",
        "\n",
        "\n",
        "def save_model(trainer, out_dir, model_name='', timestamp=None):\n",
        "    out_dir = out_dir + '/lightning_logs/version_' + str(trainer.logger.version) + '/checkpoints/'\n",
        "    if timestamp is None:\n",
        "        timestamp = time.time()\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    outfile = out_dir + '/' + model_name + '_timestamp_' + str(timestamp) + '_final.ckpt'\n",
        "    trainer.save_checkpoint(outfile, weights_only=True)\n",
        "\n",
        "    logger.info('Stored model {}.'.format(outfile))\n",
        "    return outfile\n",
        "\n",
        "\n",
        "def train_model(model, out_dir='', epochs=10, gpus=1):\n",
        "    trainer = get_trainer(gpus=gpus, out_dir=out_dir, epochs=epochs)\n",
        "    trainer.fit(model)\n",
        "    return trainer\n",
        "\n",
        "\n",
        "def get_trainer(gpus=4, is_test=False, out_dir=None, epochs=10):\n",
        "    seed_everything(42)\n",
        "    if is_test:\n",
        "        return pl.Trainer(gpus=1) if torch.cuda.is_available() else pl.Trainer(val_check_interval=100)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        trainer = pl.Trainer(gpus=gpus, deterministic=True, max_epochs=epochs, callbacks=[get_model_earlystopping_callback()],\n",
        "                             default_root_dir=out_dir, distributed_backend='ddp', checkpoint_callback=False)\n",
        "        trainer.callbacks.append(get_lr_logger())\n",
        "    else:\n",
        "        trainer = pl.Trainer(max_epochs=epochs, default_root_dir=out_dir)\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "def get_lr_logger():\n",
        "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "    return lr_monitor\n",
        "\n",
        "\n",
        "def get_model_earlystopping_callback():\n",
        "    es_clb = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        min_delta=0.001,\n",
        "        patience=3,\n",
        "        verbose=True,\n",
        "        mode='min'\n",
        "    )\n",
        "    return es_clb\n",
        "\n",
        "\n",
        "def get_models_for_evaluation(path):\n",
        "    if 'checkpoints' not in path:\n",
        "        path = path + '/checkpoints/'\n",
        "    model_files = list_files(path)\n",
        "    models = [f for f in model_files if f.endswith('final.ckpt')]\n",
        "\n",
        "    return models[0] if len(models) != 0 else None\n",
        "\n",
        "\n",
        "def list_files(in_dir):\n",
        "    files = []\n",
        "    for r, d, f in os.walk(in_dir):\n",
        "        for file in f:\n",
        "            files.append(os.path.join(r, file))\n",
        "    return files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg4-5tFCucws",
        "outputId": "d1f8d209-b7b5-4cf1-bc99-04519709751a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file __init__.py\n",
        "\n",
        "## empty file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40xt9NZKutU-",
        "outputId": "a4805fe9-c2b0-4430-fd4e-27e4ff10a73b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing __init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file evaluate.py\n",
        "\n",
        "import time\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from utils.utils import parse_args, get_reader, load_model, get_trainer, get_out_filename, write_eval_performance, get_tagset\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    timestamp = time.time()\n",
        "    sg = parse_args()\n",
        "\n",
        "    # load the dataset first\n",
        "    test_data = get_reader(file_path=sg.test, target_vocab=get_tagset(sg.iob_tagging), max_instances=sg.max_instances, max_length=sg.max_length, encoder_model=sg.encoder_model)\n",
        "\n",
        "    model, model_file = load_model(sg.model, tag_to_id=get_tagset(sg.iob_tagging))\n",
        "    trainer = get_trainer(is_test=True)\n",
        "    out = trainer.test(model, test_dataloaders=DataLoader(test_data, batch_size=sg.batch_size, collate_fn=model.collate_batch))\n",
        "\n",
        "    # use pytorch lightnings saver here.\n",
        "    eval_file = get_out_filename(sg.out_dir, model_file, prefix=sg.prefix)\n",
        "    write_eval_performance(out, eval_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S123ZzuZu4s-",
        "outputId": "024bbb5c-3dda-4c14-a48d-13f1c560e750"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file fine_tune.py\n",
        "\n",
        "import time\n",
        "\n",
        "from utils.utils import get_reader, train_model, save_model, parse_args, get_tagset, load_model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    timestamp = time.time()\n",
        "    sg = parse_args()\n",
        "    out_dir_path = sg.out_dir + '/' + sg.model_name\n",
        "\n",
        "    # load the dataset first\n",
        "    train_data = get_reader(file_path=sg.train, target_vocab=get_tagset(sg.iob_tagging), encoder_model=sg.encoder_model, max_instances=sg.max_instances, max_length=sg.max_length)\n",
        "    model, model_file = load_model(sg.model, tag_to_id=get_tagset(sg.iob_tagging), stage='finetune')\n",
        "    model.train_data = train_data\n",
        "\n",
        "    trainer = train_model(model=model, out_dir=out_dir_path, epochs=sg.epochs)\n",
        "\n",
        "    # use pytorch lightnings saver here.\n",
        "    out_model_path = save_model(trainer=trainer, out_dir=out_dir_path, model_name=sg.model_name, timestamp=timestamp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeezPMFIvEJ3",
        "outputId": "dd288caf-f084-40d5-d233-3bf2a9d52c1c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fine_tune.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file log.py\n",
        "\n",
        "import logging\n",
        "\n",
        "\n",
        "def setup_custom_logger(name, level='INFO'):\n",
        "    formatter = logging.Formatter(fmt='%(asctime)s - %(levelname)s - %(module)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    handler = logging.StreamHandler()\n",
        "    handler.setFormatter(formatter)\n",
        "\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(level)\n",
        "\n",
        "    if len(logger.handlers) == 0:\n",
        "        logger.addHandler(handler)\n",
        "    return logger\n",
        "\n",
        "\n",
        "logger = setup_custom_logger('root')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv_Qa5bgvOBe",
        "outputId": "4752a28c-3a7c-422c-d4ce-6b0b9e66cba2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing log.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file predict_tags.py\n",
        "\n",
        "import time\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils.utils import parse_args, get_reader, load_model, get_out_filename, get_tagset\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    timestamp = time.time()\n",
        "    sg = parse_args()\n",
        "\n",
        "    # load the dataset first\n",
        "    test_data = get_reader(file_path=sg.test, target_vocab=get_tagset(sg.iob_tagging), max_instances=sg.max_instances, max_length=sg.max_length)\n",
        "\n",
        "    model, model_file = load_model(sg.model, tag_to_id=get_tagset(sg.iob_tagging))\n",
        "    # use pytorch lightnings saver here.\n",
        "    eval_file = get_out_filename(sg.out_dir, model_file, prefix=sg.prefix)\n",
        "\n",
        "    test_dataloaders = DataLoader(test_data, batch_size=sg.batch_size, collate_fn=model.collate_batch, shuffle=False, drop_last=False)\n",
        "    out_str = ''\n",
        "    index = 0\n",
        "    for batch in tqdm(test_dataloaders, total=len(test_dataloaders)):\n",
        "        tokens, pred_tags = model.predict_tags(batch, tokenizer=test_data.tokenizer)\n",
        "\n",
        "        for pred_tag_inst in pred_tags:\n",
        "            out_str += '\\n'.join(pred_tag_inst)\n",
        "            out_str += '\\n\\n'\n",
        "        index += 1\n",
        "    open(eval_file, 'wt').write(out_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goFZC6yrveM0",
        "outputId": "02751848-819b-4fb2-ff45-bf2e177d4924"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict_tags.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/amzn/multiconer-baseline/main/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNWBMttLvqzU",
        "outputId": "aceea947-4cc8-46a1-da8a-8ab54bfe2ccb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-08 12:52:36--  https://raw.githubusercontent.com/amzn/multiconer-baseline/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3277 (3.2K) [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]   3.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-08 12:52:37 (20.2 MB/s) - ‘requirements.txt’ saved [3277/3277]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train_model.py\n",
        "\n",
        "import time\n",
        "\n",
        "from utils.utils import get_reader, train_model, create_model, save_model, parse_args, get_tagset\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    timestamp = time.time()\n",
        "    sg = parse_args()\n",
        "    out_dir_path = sg.out_dir + '/' + sg.model_name\n",
        "\n",
        "    # load the dataset first\n",
        "    train_data = get_reader(file_path=sg.train, target_vocab=get_tagset(sg.iob_tagging), encoder_model=sg.encoder_model, max_instances=sg.max_instances, max_length=sg.max_length)\n",
        "    dev_data = get_reader(file_path=sg.dev, target_vocab=get_tagset(sg.iob_tagging), encoder_model=sg.encoder_model, max_instances=sg.max_instances, max_length=sg.max_length)\n",
        "\n",
        "    model = create_model(train_data=train_data, dev_data=dev_data, tag_to_id=train_data.get_target_vocab(),\n",
        "                         dropout_rate=sg.dropout, batch_size=sg.batch_size, stage=sg.stage, lr=sg.lr,\n",
        "                         encoder_model=sg.encoder_model, num_gpus=sg.gpus)\n",
        "\n",
        "    trainer = train_model(model=model, out_dir=out_dir_path, epochs=sg.epochs)\n",
        "\n",
        "    # use pytorch lightnings saver here.\n",
        "    out_model_path = save_model(trainer=trainer, out_dir=out_dir_path, model_name=sg.model_name, timestamp=timestamp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFnBrnb2v3Yi",
        "outputId": "b2b8330e-c742-4dd2-a193-63d8deac8e36"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E_9IZIJwIGk",
        "outputId": "93c24b6c-0d3d-4d0a-9fc4-a9a1ab284512"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 9.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 469 kB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/bhuiyanmobasshir94/MultiCoNER/main/data/EN-English/en_dev.conll"
      ],
      "metadata": {
        "id": "XXs7EiK1xkhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/bhuiyanmobasshir94/MultiCoNER/main/data/EN-English/en_train.conll"
      ],
      "metadata": {
        "id": "mdxTJ1dAxqMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python -m train_model --train en_train.conll --dev en_dev.conll --out_dir . --model_name xlmr_ner --gpus 1 \\\n",
        "                                   --epochs 2 --encoder_model xlm-roberta-base --batch_size 64 --lr 0.0001"
      ],
      "metadata": {
        "id": "NxNgjD3WwLoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}