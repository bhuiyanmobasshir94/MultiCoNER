{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation with transformer models for named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "\n",
    "# print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "# print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "os.makedirs('data/pickles', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/pickles/data.pkl'):\n",
    "    data = joblib.load('data/pickles/data.pkl')\n",
    "else:\n",
    "    data = pd.read_csv(\"data/ner_dataset.csv\", encoding=\"latin1\")\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "    joblib.dump(data, 'data/pickles/data.pkl')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/pickles/getter.pkl'):\n",
    "    getter = joblib.load('data/pickles/getter.pkl')\n",
    "else:\n",
    "    getter = SentenceGetter(data)\n",
    "    joblib.dump(getter, 'data/pickles/getter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter.data.groupby(\"Sentence #\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter.grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"[PAD]\"]\n",
    "tags.extend(list(set(data[\"Tag\"].values)))\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "print(tags)\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"[PAD]\", \"[UNK]\"]\n",
    "words.extend(list(set(data[\"Word\"].values)))\n",
    "word2idx = {t: i for i, t in enumerate(words)}\n",
    "print(words[:20])\n",
    "# print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences, val_sentences, train_sentences = sentences[:15000], sentences[15000:20000], sentences[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAugmenter():\n",
    "    \"\"\"\n",
    "    Use the pretrained masked language model to generate more\n",
    "    labeled samples from one labeled sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_sample_tokens = 5\n",
    "        self.fill_mask = pipeline(\n",
    "            \"fill-mask\",\n",
    "            # topk=self.num_sample_tokens,\n",
    "            model=\"distilroberta-base\"\n",
    "        )\n",
    "    \n",
    "    def generate(self, sentence, num_replace_tokens=3):\n",
    "        \"\"\"Return a list of n augmented sentences.\"\"\"\n",
    "              \n",
    "        # run as often as tokens should be replaced\n",
    "        augmented_sentence = sentence.copy()\n",
    "        for i in range(num_replace_tokens):\n",
    "            # join the text\n",
    "            text = \" \".join([w[0] for w in augmented_sentence])\n",
    "            # pick a token\n",
    "            replace_token = random.choice(augmented_sentence)\n",
    "            # mask the picked token\n",
    "            masked_text = text.replace(\n",
    "                replace_token[0],\n",
    "                f\"{self.fill_mask.tokenizer.mask_token}\",\n",
    "                1            \n",
    "            )\n",
    "            # fill in the masked token with Bert\n",
    "            res = self.fill_mask(masked_text)[random.choice(range(self.num_sample_tokens))]\n",
    "            # create output samples list\n",
    "            tmp_sentence, augmented_sentence = augmented_sentence.copy(), []\n",
    "            for w in tmp_sentence:\n",
    "                if w[0] == replace_token[0]:\n",
    "                    augmented_sentence.append((res[\"token_str\"].replace(\"Ä \", \"\"), w[1], w[2]))\n",
    "                else:\n",
    "                    augmented_sentence.append(w)\n",
    "            text = \" \".join([w[0] for w in augmented_sentence])\n",
    "        return [sentence, augmented_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/pickles/augmenter.pkl'):\n",
    "    augmenter = joblib.load('data/pickles/augmenter.pkl')\n",
    "else:\n",
    "    augmenter = TransformerAugmenter()\n",
    "    joblib.dump(augmenter, 'data/pickles/augmenter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_sentences = augmenter.generate(train_sentences[12], num_replace_tokens=7); augmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tqdm issue solved by following command:\n",
    "```\n",
    "conda install -c conda-forge ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/pickles/augmented_sentences.pkl'):\n",
    "    augmented_sentences = joblib.load('data/pickles/augmented_sentences.pkl')\n",
    "else:\n",
    "    # only use a thousand senteces with augmentation\n",
    "    n_sentences = 1000\n",
    "    augmented_sentences = []\n",
    "    for sentence in tqdm(train_sentences[:n_sentences]):\n",
    "        augmented_sentences.extend(augmenter.generate(sentence, num_replace_tokens=7))\n",
    "    joblib.dump(augmented_sentences, 'data/pickles/augmented_sentences.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(augmented_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import f1\n",
    "from torchmetrics.functional import accuracy\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningLSTMTagger(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(LightningLSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(len(word2idx), embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, len(tag2idx))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = lstm_out\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = y_hat.permute(0, 2, 1)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        result = pl.TrainResult(minimize=loss)\n",
    "        result.log('f1', f1_score(torch.argmax(y_hat, dim=1), y), prog_bar=True)\n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = y_hat.permute(0, 2, 1)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        result = pl.EvalResult()\n",
    "        result.log('val_f1', f1_score(torch.argmax(y_hat, dim=1), y), prog_bar=True)\n",
    "        return result\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = y_hat.permute(0, 2, 1)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return {'test_f1':  f1_score(torch.argmax(y_hat, dim=1), y)}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9721ba0f845b50d5928f636648bd953dbefce528341367a1a1a06e9aa2bb129"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('multiconer': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
