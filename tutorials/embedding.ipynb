{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mobasshirbhuia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12443 sentences\n"
     ]
    }
   ],
   "source": [
    "# Split text into sentences\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "book = urllib.request.urlopen(url=\"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/harrypotter.txt\")\n",
    "sentences = tokenizer.tokenize(str(book.read()))\n",
    "print (f\"{len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'Harry Potter and the Deathly Hallows By J. K. Rowling\\\\r\\\\n\\\\r\\\\nChapter One The Dark Lord Ascending\\\\r\\\\nThe two men appeared out of nowhere, a few yards apart in the narrow, moonlit lane.\",\n",
       " \"For a second they stood quite still, wands directed at each other\\\\'s chests; then, recognizing each other, they stowed their wands beneath their cloaks and started walking briskly in the same direction.\",\n",
       " '\"News?\"',\n",
       " 'asked the taller of the two.',\n",
       " '\"The best,\" replied Severus Snape.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Conditional preprocessing on our text.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # Separate into word tokens\n",
    "    text = text.split(\" \")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snape nodded, but did not elaborate.\n",
      "['snape', 'nodded', 'but', 'did', 'not', 'elaborate']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess sentences\n",
    "print (sentences[11])\n",
    "sentences = [preprocess(sentence) for sentence in sentences]\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 3 # Ignores all words with total frequency lower than this\n",
    "SKIP_GRAM = 1 # 0 = CBOW\n",
    "NEGATIVE_SAMPLING = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4937, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Super fast because of optimized C code under the hood\n",
    "w2v = Word2Vec(\n",
    "    sentences=sentences, vector_size=EMBEDDING_DIM,\n",
    "    window=WINDOW, min_count=MIN_COUNT,\n",
    "    sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)\n",
    "print (w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21710205,  0.237692  ,  0.0592954 , -0.07985489,  0.09517752,\n",
       "       -0.28969437,  0.47183746,  0.26759276, -0.3999157 , -0.02191537,\n",
       "       -0.14402515,  0.03171716, -0.08802347,  0.30826014,  0.12235343,\n",
       "       -0.03062629,  0.29036772,  0.1026497 , -0.30932784, -0.30340514,\n",
       "        0.34278795,  0.27602437,  0.4049073 , -0.20330477, -0.16292319,\n",
       "        0.03444757, -0.4659836 , -0.31702667, -0.22744374, -0.00952164,\n",
       "       -0.11662934,  0.4538719 ,  0.27198884, -0.45004848, -0.03083939,\n",
       "        0.28689602, -0.43850502, -0.09756269, -0.10509751, -0.2724514 ,\n",
       "        0.20651884, -0.02037722, -0.16187604,  0.1485245 ,  0.20171   ,\n",
       "        0.26071823, -0.3466709 , -0.3055512 , -0.14942464,  0.09217129,\n",
       "       -0.01269889,  0.0937115 ,  0.03608094, -0.04900232, -0.07322596,\n",
       "       -0.19003938,  0.20401101,  0.02362426, -0.07051799,  0.0512586 ,\n",
       "        0.07159985, -0.25800744, -0.03183749,  0.12028711, -0.19699335,\n",
       "        0.1821521 ,  0.2675212 ,  0.3730585 , -0.39576593, -0.15367903,\n",
       "       -0.33246294, -0.13178891,  0.17768891,  0.34601718,  0.3854138 ,\n",
       "       -0.28394386,  0.14186993,  0.07722133, -0.3394239 , -0.02916709,\n",
       "       -0.31999657,  0.23992725, -0.2162449 ,  0.1873507 ,  0.24668248,\n",
       "        0.2644318 ,  0.7772059 , -0.19703187,  0.2704896 , -0.14410132,\n",
       "        0.40742958,  0.34789947,  0.3525137 , -0.17889075,  0.39507222,\n",
       "       -0.1398038 ,  0.5470605 , -0.01822658,  0.11162423, -0.0723566 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector for each word\n",
    "w2v.wv.get_vector(\"potter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('forehead', 0.9125005006790161),\n",
       " ('pain', 0.8974847793579102),\n",
       " ('mouth', 0.8922736644744873),\n",
       " ('throat', 0.8868098258972168),\n",
       " ('prickling', 0.8744701743125916)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get nearest neighbors (excluding itself)\n",
    "w2v.wv.most_similar(positive=\"scar\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading\n",
    "w2v.wv.save_word2vec_format(\"model.bin\", binary=True)\n",
    "w2v = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=4937, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Super fast because of optimized C code under the hood\n",
    "ft = FastText(sentences=sentences, vector_size=EMBEDDING_DIM,\n",
    "              window=WINDOW, min_count=MIN_COUNT,\n",
    "              sg=SKIP_GRAM, negative=NEGATIVE_SAMPLING)\n",
    "print (ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'scarring' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m4/36907pl92_b4mbw7y0f50bzr0000gn/T/ipykernel_23451/1941812172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This word doesn't exist so the word2vec model will error out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"scarring\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/mwml_nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/mwml_nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/mwml_nlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'scarring' not present\""
     ]
    }
   ],
   "source": [
    "# This word doesn't exist so the word2vec model will error out\n",
    "w2v.most_similar(positive=\"scarring\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dabbing', 0.9921090602874756),\n",
       " ('bulging', 0.9910171627998352),\n",
       " ('shuddering', 0.9908440709114075),\n",
       " ('piercing', 0.9902076125144958),\n",
       " ('swimming', 0.9901530742645264)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText will use n-grams to embed an OOV word\n",
    "ft.wv.most_similar(positive=\"scarring\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and loading\n",
    "ft.wv.save(\"model.bin\")\n",
    "ft = KeyedVectors.load(\"model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(words, embeddings, pca_results):\n",
    "    for word in words:\n",
    "        index = embeddings.index_to_key.index(word)\n",
    "        plt.scatter(pca_results[index, 0], pca_results[index, 1])\n",
    "        plt.annotate(word, xy=(pca_results[index, 0], pca_results[index, 1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file (may take ~3-5 minutes)\n",
    "resp = urlopen(\"http://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "zipfile = ZipFile(BytesIO(resp.read()))\n",
    "zipfile.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mobasshirbhuia/Desktop/MultiCoNER/tutorials/glove.6B.100d.txt'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write embeddings to file\n",
    "embeddings_file = \"glove.6B.{0}d.txt\".format(EMBEDDING_DIM)\n",
    "zipfile.extract(embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: the\n",
      "embedding:\n",
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "embedding dim: 100\n"
     ]
    }
   ],
   "source": [
    "# Preview of the GloVe embeddings file\n",
    "with open(embeddings_file, \"r\") as fp:\n",
    "    line = next(fp)\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embedding = np.asarray(values[1:], dtype='float32')\n",
    "    print (f\"word: {word}\")\n",
    "    print (f\"embedding:\\n{embedding}\")\n",
    "    print (f\"embedding dim: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mwml_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save GloVe embeddings to local directory in word2vec format\n",
    "word2vec_output_file = \"{0}.word2vec\".format(embeddings_file)\n",
    "glove2word2vec(embeddings_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings (may take a minute)\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755736470222473),\n",
       " ('daughter', 0.6594555974006653),\n",
       " ('princess', 0.6520534157752991)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (king - man) + woman = ?\n",
    "# king - man = ? -  woman\n",
    "glove.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gohan', 0.7246543169021606),\n",
       " ('bulma', 0.6497019529342651),\n",
       " ('raistlin', 0.644360363483429),\n",
       " ('skaar', 0.6316742897033691),\n",
       " ('guybrush', 0.6231325268745422)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get nearest neighbors (excluding itself)\n",
    "glove.most_similar(positive=\"goku\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality for plotting\n",
    "X = glove[glove.index_to_key]\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize\n",
    "plot_embeddings(\n",
    "    words=[\"king\", \"queen\", \"man\", \"woman\"], embeddings=glove,\n",
    "    pca_results=pca_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nurse', 0.7735227942466736),\n",
       " ('physician', 0.7189430594444275),\n",
       " ('doctors', 0.6824329495429993),\n",
       " ('patient', 0.6750682592391968),\n",
       " ('dentist', 0.6726033091545105)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias in embeddings\n",
    "glove.most_similar(positive=[\"woman\", \"doctor\"], negative=[\"man\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=1234):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device(\"cuda\" if (\n",
    "    torch.cuda.is_available() and cuda) else \"cpu\")\n",
    "torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sharon Accepts Plan to Reduce Gaza Army Operat...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Internet Key Battleground in Wildlife Crime Fight</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>July Durable Good Orders Rise 1.7 Percent</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Growing Signs of a Slowing on Wall Street</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The New Faces of Reality TV</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  category\n",
       "0  Sharon Accepts Plan to Reduce Gaza Army Operat...     World\n",
       "1  Internet Key Battleground in Wildlife Crime Fight  Sci/Tech\n",
       "2          July Durable Good Orders Rise 1.7 Percent  Business\n",
       "3          Growing Signs of a Slowing on Wall Street  Business\n",
       "4                        The New Faces of Reality TV     World"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/news.csv\"\n",
    "df = pd.read_csv(url, header=0) # load\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffle\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mobasshirbhuia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "print (STOPWORDS[:5])\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stopwords=STOPWORDS):\n",
    "    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub(\"\", text)\n",
    "\n",
    "    # Remove words in paranthesis\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text) # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great week nyse'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "text = \"Great week for the NYSE!\"\n",
    "preprocess(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n",
      "\n",
      "sharon accepts plan reduce gaza army operation haaretz says\n"
     ]
    }
   ],
   "source": [
    "# Apply to dataframe\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\n",
    "print (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, train_size):\n",
    "    \"\"\"Split dataset into data splits.\"\"\"\n",
    "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = preprocessed_df[\"title\"].values\n",
    "y = preprocessed_df[\"category\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (84000,), y_train: (84000,)\n",
      "X_val: (18000,), y_val: (18000,)\n",
      "X_test: (18000,), y_test: (18000,)\n",
      "Sample point: china battles north korea nuclear talks → World\n"
     ]
    }
   ],
   "source": [
    "# Create data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X=X, y=y, train_size=TRAIN_SIZE)\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print (f\"Sample point: {X_train[0]} → {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(object):\n",
    "    \"\"\"Label encoder for tag labels.\"\"\"\n",
    "    def __init__(self, class_to_index={}):\n",
    "        self.class_to_index = class_to_index\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
    "\n",
    "    def fit(self, y):\n",
    "        classes = np.unique(y)\n",
    "        for i, class_ in enumerate(classes):\n",
    "            self.class_to_index[class_] = i\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "        return self\n",
    "\n",
    "    def encode(self, y):\n",
    "        encoded = np.zeros((len(y)), dtype=int)\n",
    "        for i, item in enumerate(y):\n",
    "            encoded[i] = self.class_to_index[item]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, y):\n",
    "        classes = []\n",
    "        for i, item in enumerate(y):\n",
    "            classes.append(self.index_to_class[item])\n",
    "        return classes\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, \"w\") as fp:\n",
    "            contents = {'class_to_index': self.class_to_index}\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, \"r\") as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "NUM_CLASSES = len(label_encoder)\n",
    "label_encoder.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[0]: World\n",
      "y_train[0]: 3\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to tokens\n",
    "print (f\"y_train[0]: {y_train[0]}\")\n",
    "y_train = label_encoder.encode(y_train)\n",
    "y_val = label_encoder.encode(y_val)\n",
    "y_test = label_encoder.encode(y_test)\n",
    "print (f\"y_train[0]: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: [21000 21000 21000 21000]\n",
      "weights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount(y_train)\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (f\"counts: {counts}\\nweights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from more_itertools import take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, char_level, num_tokens=None,\n",
    "                 pad_token=\"<PAD>\", oov_token=\"<UNK>\",\n",
    "                 token_to_index=None):\n",
    "        self.char_level = char_level\n",
    "        self.separator = \"\" if self.char_level else \" \"\n",
    "        if num_tokens: num_tokens -= 2 # pad + unk tokens\n",
    "        self.num_tokens = num_tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.oov_token = oov_token\n",
    "        if not token_to_index:\n",
    "            token_to_index = {pad_token: 0, oov_token: 1}\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Tokenizer(num_tokens={len(self)})>\"\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        if not self.char_level:\n",
    "            texts = [text.split(\" \") for text in texts]\n",
    "        all_tokens = [token for text in texts for token in text]\n",
    "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
    "        self.min_token_freq = counts[-1][1]\n",
    "        for token, count in counts:\n",
    "            index = len(self)\n",
    "            self.token_to_index[token] = index\n",
    "            self.index_to_token[index] = token\n",
    "        return self\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            if not self.char_level:\n",
    "                text = text.split(\" \")\n",
    "            sequence = []\n",
    "            for token in text:\n",
    "                sequence.append(self.token_to_index.get(\n",
    "                    token, self.token_to_index[self.oov_token]))\n",
    "            sequences.append(np.asarray(sequence))\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = []\n",
    "            for index in sequence:\n",
    "                text.append(self.index_to_token.get(index, self.oov_token))\n",
    "            texts.append(self.separator.join([token for token in text]))\n",
    "        return texts\n",
    "\n",
    "    def save(self, fp):\n",
    "        with open(fp, \"w\") as fp:\n",
    "            contents = {\n",
    "                \"char_level\": self.char_level,\n",
    "                \"oov_token\": self.oov_token,\n",
    "                \"token_to_index\": self.token_to_index\n",
    "            }\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        with open(fp, \"r\") as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Tokenizer(num_tokens=5000)>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokenizer = Tokenizer(char_level=False, num_tokens=5000)\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<PAD>', 0), ('<UNK>', 1), ('39', 2), ('b', 3), ('gt', 4)]\n",
      "least freq token's freq: 14\n"
     ]
    }
   ],
   "source": [
    "# Sample of tokens\n",
    "print (take(5, tokenizer.token_to_index.items()))\n",
    "print (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to indices:\n",
      "  (preprocessed) → china battles north korea nuclear talks\n",
      "  (tokenized) → [  16 1491  285  142  114   24]\n"
     ]
    }
   ],
   "source": [
    "# Convert texts to sequences of indices\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "print (\"Text to indices:\\n\"\n",
    "    f\"  (preprocessed) → {preprocessed_text}\\n\"\n",
    "    f\"  (tokenized) → {X_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 1, 6, 5, 6]])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "vocab_size = 10\n",
    "x = torch.randint(high=vocab_size, size=(1,5))\n",
    "print (x)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "embeddings = nn.Embedding(embedding_dim=100, num_embeddings=vocab_size)\n",
    "print (embeddings.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 100])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed the input\n",
    "embeddings(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_seq_len=0):\n",
    "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
    "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
    "    padded_sequences = np.zeros((len(sequences), max_seq_len))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        padded_sequences[i][:len(sequence)] = sequence\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "[[1.600e+01 1.491e+03 2.850e+02 1.420e+02 1.140e+02 2.400e+01]\n",
      " [1.445e+03 2.300e+01 6.560e+02 2.197e+03 1.000e+00 0.000e+00]\n",
      " [1.200e+02 1.400e+01 1.955e+03 1.005e+03 1.529e+03 4.014e+03]]\n"
     ]
    }
   ],
   "source": [
    "# 2D sequences\n",
    "padded = pad_sequences(X_train[0:3])\n",
    "print (padded.shape)\n",
    "print (padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZES = list(range(1, 4)) # uni, bi and tri grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, max_filter_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_filter_size = max_filter_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Dataset(N={len(self)})>\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return [X, y]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"Processing on a batch.\"\"\"\n",
    "        # Get inputs\n",
    "        batch = np.array(batch)\n",
    "        X = batch[:, 0]\n",
    "        y = batch[:, 1]\n",
    "\n",
    "        # Pad sequences\n",
    "        X = pad_sequences(X)\n",
    "\n",
    "        # Cast\n",
    "        X = torch.LongTensor(X.astype(np.int32))\n",
    "        y = torch.LongTensor(y.astype(np.int32))\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n",
    "            shuffle=shuffle, drop_last=drop_last, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:\n",
      "  Train dataset:<Dataset(N=84000)>\n",
      "  Val dataset: <Dataset(N=18000)>\n",
      "  Test dataset: <Dataset(N=18000)>\n",
      "Sample point:\n",
      "  X: [  16 1491  285  142  114   24]\n",
      "  y: 3\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "max_filter_size = max(FILTER_SIZES)\n",
    "train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=max_filter_size)\n",
    "val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=max_filter_size)\n",
    "test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=max_filter_size)\n",
    "print (\"Datasets:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {train_dataset[0][0]}\\n\"\n",
    "    f\"  y: {train_dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch:\n",
      "  X: [64, 14]\n",
      "  y: [64]\n",
      "Sample point:\n",
      "  X: tensor([  16, 1491,  285,  142,  114,   24,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n",
      "  y: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mwml_nlp/lib/python3.7/site-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {batch_X[0]}\\n\"\n",
    "    f\"  y: {batch_y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0e33297658497c7adaeaa6c7fec3765fbf7bdb630e1100848edd6d26b08ff83"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('mwml_nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
